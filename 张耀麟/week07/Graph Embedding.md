# Graph Embedding

Graph Embedding 是一种将图结构数据转换为低维向量表示的技术，其核心思想与 word2vec 等词嵌入方法类似。Embedding 技术的主要作用是将无法直接作为计算机输入的非结构化数据（如图中的节点、边等）转化为数值化的向量表示，从而使计算机能够有效处理和分析这类数据。

## DeepWalk

在介绍DeepWalk之前，先简单介绍一下word2vec。

word2vec 通过两种主要的训练策略来学习词汇的向量表示：CBOW（Continuous Bag-of-Words）和 Skip-gram。

CBOW 利用上下文词汇预测中心词，而 Skip-gram 则通过中心词预测其上下文词汇。

这两种方法都在局部窗口内捕捉词汇之间的语义关系。



正如word2vec中的假设一样，可以假设图中一个随机节点与周围节点之间存在联系。

具体而言，如果能够将图中的节点序列转化为类似自然语言中的句子，那么就可以利用 word2vec 的方法学习节点的向量表示。

因此，我们可以图中多个节点通过排序构建成一个序列，形成类似与word2vec中的句子，并且使用word2vec中的方法来获取每个节点的向量表示。



基于这一思想，Graph Embedding 领域的开创性论文 **《DeepWalk: Online Learning of Social Representations》** 提出了一种有效的节点嵌入方法，其主要流程如下：

1. **构建节点序列**

   DeepWalk 通过随机游走（Random Walk）生成节点序列。

   假设有一个喝酒的醉汉，从图中随机一个节点出发，朝着任意一个相邻节点的方向随机游走。

   这位醉汉可能向前走，也可能往回走。当他每次到达一个节点就将该节点记录下来，直到到达设定的最长长度T。

   在这个过程中，我们就采集到了这个图的部分节点序列表示，换言之，我们获得了许多”句子“。

2. **向量表示学习：Skip-gram**

   与word2vec类似，对于随机获取到的序列，可以通过Skip-gram获取”节点 -> 向量“得到一个将节点映射为低维向量的权重矩阵。



## Node2vec

待补充



## Line

待补充